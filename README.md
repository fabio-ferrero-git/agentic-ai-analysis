## The Autonomy Equation: How Agentic AI Reshapes Trust and Workload in Productivity Applications

---

# User Study Analysis

This project contains the data, analysis scripts, and notebooks used for analyzing the results 
of a user study comparing different systems/conditions. 

The analysis covers participant demographics, questionnaire responses, user activity logs, and 
performance metrics.


## Prerequisites
The code was developed using Python 3.12.9
<br>
Required Python packages can be installed via:
```bash
pip install -r requirements.txt
```

---
# How to Run
The analysis is performed through **three main Jupyter Notebooks**, each focusing on a different aspect of the study:

1.  **Choose a Notebook**: Select one of the following notebooks based on the analysis you wish to perform:
    *   `demographic_analysis.ipynb`: For analyzing participant demographic data.
    *   `questionnaire_analysis.ipynb`: For analyzing questionnaire responses and statistics.
    *   `user_activity_analysis.ipynb`: For analyzing user activity, interactions, and performance metrics during the task.

2.  **Run the Notebook**: Execute the cells within the chosen notebook sequentially.

### Note:
**First Run & Pre-processing**:

*   If this is the first time running any of the analysis notebooks, the necessary pre-processing steps **will be executed automatically**. This involves running `analysis_functions/pre_processing.py`.
*   The script will read the raw data from the `/data/` directory, perform calculations and aggregations (e.g., calculating questionnaires scores, counting actions, etc.), and save the results as JSON files in the `/data/pre_processed/` directory.
*   This initial pre-processing step should take around 10/15 seconds.

**Subsequent Runs**:

*   Once the `/data/pre_processed/` directory has been populated, subsequent runs of any other notebooks will load the pre-processed data directly from these JSON files.
*   This speeds up the analysis process, as the data pre-processing is skipped.

---
## Project Structure

The project is organized into the following directories and files:
```
/
├── /data    # Contains raw, and pre-processed data files
│   │
│   ├── /demographic    # Participant demographic data 
│   │   ├── age.csv
│   │   ├── ethnicity.csv
│   │   ├── gender.csv
│   │   ├── language.csv
│   │   └── nationality.csv
│   │
│   ├── /free_text    # Categorized free-text responses from questionnaires
│   │   └── free_text_answers.csv
│   │
│   ├── /precision_recall    # Precision and recall metrics per system condition
│   │   ├── pr_rec_sys0.csv
│   │   ├── pr_rec_sys1.csv
│   │   └── pr_rec_sys2.csv
│   │
│   ├── /pre_processed    # (Generated by pre_processing.py) 
│   │                       Intermediate aggregated data
│   │
│   ├── csv_df_test_log.csv      # Raw user interaction event logs from the study
│   ├── csv_df_user_model.csv    # Users metadata
│   ├── scenarios.json           # Details of the experimental scenarios
│   └── study_question.json      # Text content of all questions used in the study questionnaires
│   
├── /analysis_functions   # Contains core Python modules for data processing and analysis
│   ├── logs.py
│   ├── pre_processing.py
│   ├── statistic.py
│   └── utils.py
│
├── /llm_prompts         # Contains LLM Prompts
│
├── demographic_analysis.ipynb      # Jupyter notebook for analyzing participant demographic data
├── questionnaire_analysis.ipynb    # Jupyter notebook for analyzing questionnaire responses
└── user_activity_analysis.ipynb    # Jupyter notebook for analyzing user activity and performance metrics
```

---
## File Descriptions

### `/analysis_functions/`
This directory contains Python modules with reusable functions and classes for the analysis pipeline.

*   **`logs.py`**:
    *   Provides a framework for parsing, structuring, and analyzing user study log data.
    *   Defines core classes like `Event`, `LogEvent`, `AnswersEvent`, `Timeline`, and `Logs`.
    *   Handles the loading of raw log data (`csv_df_test_log.csv`) and user metadata (`csv_df_user_model.csv`).
    *   Structures event data chronologically into `Timeline` objects for each user.
    *   Includes methods for processing questionnaire answers (`process_answer`) based on defined log names (`questionnaire_log_name`).
    *   Offers various methods within the `Timeline` class to query and retrieve specific events, logs, or answers based on names, timestamps, or custom functions.


*   **`pre_processing.py`**:
    *   Contains functions to perform initial data aggregation and pre-processing steps.
    *   Reads raw log data (`csv_df_test_log.csv`), user model data (`csv_df_user_model.csv`), scenario details (`scenarios.json`), and study questions (`study_question.json`) using the `Logs` class from `logs.py`.
    *   Utilizes functions from `utils.py` to extract information like study durations, questionnaire durations and results, free-text answers, and user actions.
    *   Calculates aggregated metrics per user and condition (e.g., number of actions, reply rates, user vs. LLM actions).
    *   Saves the aggregated and pre-processed data into JSON files within the `/data/pre_processed/` directory (e.g., `study_durations.json`, `questionnaires_results.json`, `n_actions_in_5min.json`).


*   **`statistic.py`**:
    *   Provides functions for conducting statistical analysis on the pre-processed data.
    *   Includes functions for analyzing user activity metrics
    *   Contains functions for analyzing questionnaire data
    *   Implements various statistical tests:
        *   Non-parametric tests for comparing groups (Kruskal-Wallis, Mann-Whitney).
        *   Equivalence testing (Non-parametric TOST).
    *   Includes functions for performing linear regression and comparing correlation coefficients.
    *   Provides helper functions for formatting p-values with significance indicators and Bonferroni-Holm correction.
    *   Contains functions for calculating performance metrics like precision and recall based on action counts.
    *   Includes functions specifically designed to analyze aggregated results from questionnaires.

  
*   **`utils.py`**:
    *   Contains various utility functions and constants and mappings used across the analysis scripts.
    *   Includes helper functions for:
    *   Includes functions specifically for summarizing demographic data from CSV files.



### `/data/`

This directory stores all data related to the study.

*   **`/demographic/`**: Contains raw demographic data for study participants.
*   **`/free_text/`**: Contains categorized free-text responses provided by participants
*   **`/precision_recall/`**: Contains calculated precision and recall metrics for tasks performed within each system condition.
*   **`/pre_processed/`**: Contains JSON files generated by `pre_processing.py`, holding aggregated data ready for statistical analysis.
*   `csv_df_test_log.csv`: The primary raw data file containing timestamped event logs for all user interactions during the study.
*   `csv_df_user_model.csv`: Contains metadata about each participant.
*   `scenarios.json`: A JSON file defining the structure and content of the different experimental scenarios presented to users (e.g., base emails, follow-ups).
*   `study_question.json`: A JSON file mapping internal questionnaire names to the list of actual question texts presented to the participants.


### Root Directory Files

*   **`demographic_analysis.ipynb`**: A Jupyter Notebook dedicated to loading, analyzing, and potentially visualizing the participant demographic data found in `/data/demographic/`.


*   **`questionnaire_analysis.ipynb`**: A Jupyter Notebook focused on analyzing the questionnaire data. It loads pre-processed questionnaire results and durations from `/data/pre_processed/` (generated by `pre_processing.py`) and applies statistical tests and visualizations using functions from `statistic.py` and `utils.py`.


*   **`user_activity_analysis.ipynb`**: A Jupyter Notebook for analyzing user behavior and performance during the interactive task phase. It loads pre-processed activity data (e.g., action counts, reply rates, precision/recall) from `/data/pre_processed/` and `/data/precision_recall/` and uses functions from `statistic.py` and `utils.py` for analysis and visualization.






